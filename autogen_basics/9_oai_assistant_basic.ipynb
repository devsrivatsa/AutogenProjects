{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import autogen\n",
    "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.WARNING)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-4-1106-preview\",\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unlike the agents in autogen, GPTAssistantAgent from open ai will take the function schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_schema = {\n",
    "    \"name\": \"somename\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"argument\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": ()\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"argument\"]\n",
    "    },\n",
    "    \"description\": \"some description\"\n",
    "}\n",
    "\n",
    "def somefunction(argument):\n",
    "    pass\n",
    "\n",
    "llm_config={\n",
    "    # \"cache_seed\":42,\n",
    "    # \"temperature\":0,\n",
    "    \"config_list\":config_list,\n",
    "    \"assistant_id\": os.environ.get(\"ASSISTANY_ID\", None),\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": api_schema\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT Assistant only supports one OpenAI client. Using the first client in the list.\n",
      "assistant_id was None, creating a new assistant\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\n",
    "gpt_assistant = GPTAssistantAgent(\n",
    "    name = \"assistant\",\n",
    "    instructions = autogen.AssistantAgent.DEFAULT_SYSTEM_MESSAGE if not instruction else instruction,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    "    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy.initiate_chat(gpt_assistant, message=\"Top 10 developers with the most followers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With code interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT Assistant only supports one OpenAI client. Using the first client in the list.\n",
      "assistant_id was None, creating a new assistant\n"
     ]
    }
   ],
   "source": [
    "gpt_assistant = GPTAssistantAgent(\n",
    "    name=\"Coder Assistant\",\n",
    "    llm_config={\n",
    "        \"tools\": [\n",
    "            {\"type\":\"code_interpreter\"}\n",
    "        ],\n",
    "        \"config_list\": config_list\n",
    "    },\n",
    "    instructions=\"You are an expert at solving math problems. Write code and run it to solve math problems. Reply TERMINATE when the task is solved and there is no problem.\"\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
    "    },\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy.initiate_chat(\n",
    "#     gpt_assistant, \n",
    "#     message=\"If $725x + 727y = 1500$ and $729x+ 731y = 1508$, what is the value of $x - y$ ?\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting with code interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT Assistant only supports one OpenAI client. Using the first client in the list.\n",
      "assistant_id was None, creating a new assistant\n"
     ]
    }
   ],
   "source": [
    "gpt_assistant = GPTAssistantAgent(\n",
    "    name=\"Coder Assistant\",\n",
    "    llm_config={\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"type\": \"code_interpreter\"\n",
    "            }\n",
    "        ],\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    "    instructions=\"You are an expert at writing python code to solve problems. Reply TERMINATE when the task is solved and there is no problem.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy.initiate_chat(\n",
    "#     gpt_assistant, \n",
    "#     message=\"Draw a line chart to show the population trend in US. Show how you solved it with code.\", \n",
    "#     clear_history=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyautogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
