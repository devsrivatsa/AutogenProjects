{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": \"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\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-1106-preview\",\n",
    "        \"api_key\": \"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\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config={\n",
    "    # \"cache_seed\":42,\n",
    "    \"temperature\":0,\n",
    "    \"config_list\":config_list,\n",
    "    \"timeout\": 120,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "    human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"coder\",\n",
    "    llm_config=llm_config\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in sofrware product ideas\",\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, coder, pm], \n",
    "    messages=[], \n",
    "    max_round=12\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion earlier. The ArXiv API does not provide a direct way to summarize the papers. However, we can use a text summarization model to summarize the abstracts of the papers. \n",
      "\n",
      "We will use the transformers library from Hugging Face, which provides a wide range of pre-trained models for various tasks, including text summarization. We will use the 'sshleifer/distilbart-cnn-12-6' model, which is a DistilBART model fine-tuned for summarization tasks.\n",
      "\n",
      "First, you need to install the transformers library. Please run the following command to install it:\n",
      "\n",
      "```sh\n",
      "pip install transformers\n",
      "```\n",
      "\n",
      "After installing the transformers library, please run the following Python code:\n",
      "\n",
      "```python\n",
      "# filename: research_paper_summary.py\n",
      "\n",
      "import requests\n",
      "import feedparser\n",
      "from transformers import pipeline\n",
      "\n",
      "def search_papers(topic):\n",
      "    url = 'http://export.arxiv.org/api/query?search_query=all:' + topic.replace(' ', '%20') + '&sortBy=submittedDate&sortOrder=descending'\n",
      "    data = feedparser.parse(url)\n",
      "    return data.entries\n",
      "\n",
      "def summarize_text(text):\n",
      "    summarizer = pipeline(\"summarization\")\n",
      "    summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
      "    return summary[0]['summary_text']\n",
      "\n",
      "topic = \"gpt-4\"\n",
      "\n",
      "papers = search_papers(topic)\n",
      "print(f\"Topic: {topic}\")\n",
      "for paper in papers[:1]:  # Get only the latest paper\n",
      "    print(f\"Title: {paper['title']}\")\n",
      "    print(f\"Authors: {', '.join([author['name'] for author in paper['authors']])}\")\n",
      "    summary = summarize_text(paper['summary'])\n",
      "    print(f\"Summary: {summary}\")\n",
      "    print(\"\\n\")\n",
      "```\n",
      "\n",
      "This script will print the title, authors, and a summary of less than 50 words of the latest research paper on the given topic from the ArXiv repository.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-21 17:54:40] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4a86d23e252138a55fac78f9a44ff92b in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4a86d23e252138a55fac78f9a44ff92b in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4a86d23e252138a55fac78f9a44ff92b in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 21 Nov 2023 22:54:40 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '134', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '144949', 'x-ratelimit-remaining-tokens_usage_based': '144949', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '2.02s', 'x-ratelimit-reset-tokens_usage_based': '2.02s', 'x-request-id': '4a86d23e252138a55fac78f9a44ff92b', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829c840e9c2736d6-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py:224\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39;49mcreate(request_timeout\u001b[39m=\u001b[39;49mrequest_timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    225\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m    226\u001b[0m     ServiceUnavailableError,\n\u001b[1;32m    227\u001b[0m     APIConnectionError,\n\u001b[1;32m    228\u001b[0m ):\n\u001b[1;32m    229\u001b[0m     \u001b[39m# transient error\u001b[39;00m\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    140\u001b[0m (\n\u001b[1;32m    141\u001b[0m     deployment_id,\n\u001b[1;32m    142\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m     url,\n\u001b[1;32m    158\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    289\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m     method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m )\n\u001b[0;32m--> 299\u001b[0m resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4a86d23e252138a55fac78f9a44ff92b in your email.) {\n  \"error\": {\n    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4a86d23e252138a55fac78f9a44ff92b in your email.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4a86d23e252138a55fac78f9a44ff92b in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 21 Nov 2023 22:54:40 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '134', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '144949', 'x-ratelimit-remaining-tokens_usage_based': '144949', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '2.02s', 'x-ratelimit-reset-tokens_usage_based': '2.02s', 'x-request-id': '4a86d23e252138a55fac78f9a44ff92b', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829c840e9c2736d6-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/autogen_basics/4_groupChat.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/autogen_basics/4_groupChat.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/autogen_basics/4_groupChat.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     manager,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/autogen_basics/4_groupChat.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFind a latest paper about gpt-4 on arxiv and find its potential applications in software.\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/autogen_basics/4_groupChat.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/agentchat/groupchat.py:162\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[39m# select the next speaker\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     speaker \u001b[39m=\u001b[39m groupchat\u001b[39m.\u001b[39;49mselect_speaker(speaker, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    163\u001b[0m     \u001b[39m# let the speaker speak\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     reply \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39mgenerate_reply(sender\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/agentchat/groupchat.py:91\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m     87\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     88\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGroupChat is underpopulated with \u001b[39m\u001b[39m{\u001b[39;00mn_agents\u001b[39m}\u001b[39;00m\u001b[39m agents. Direct communication would be more efficient.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     90\u001b[0m selector\u001b[39m.\u001b[39mupdate_system_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_speaker_msg(agents))\n\u001b[0;32m---> 91\u001b[0m final, name \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mgenerate_oai_reply(\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessages\n\u001b[1;32m     93\u001b[0m     \u001b[39m+\u001b[39;49m [\n\u001b[1;32m     94\u001b[0m         {\n\u001b[1;32m     95\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     96\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRead the above conversation. Then select the next role from \u001b[39;49m\u001b[39m{\u001b[39;49;00m[agent\u001b[39m.\u001b[39;49mname\u001b[39m \u001b[39;49m\u001b[39mfor\u001b[39;49;00m\u001b[39m \u001b[39;49magent\u001b[39m \u001b[39;49m\u001b[39min\u001b[39;49;00m\u001b[39m \u001b[39;49magents]\u001b[39m}\u001b[39;49;00m\u001b[39m to play. Only return the role.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     97\u001b[0m         }\n\u001b[1;32m     98\u001b[0m     ]\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m final:\n\u001b[1;32m    101\u001b[0m     \u001b[39m# i = self._random.randint(0, len(self._agent_names) - 1)  # randomly pick an id\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_agent(last_speaker, agents)\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    801\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    804\u001b[0m         context,\n\u001b[1;32m    805\u001b[0m         use_cache,\n\u001b[1;32m    806\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    807\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    808\u001b[0m     )\n\u001b[1;32m    809\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    810\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py:239\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[39m# transient error\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrying in \u001b[39m\u001b[39m{\u001b[39;00mretry_wait_time\u001b[39m}\u001b[39;00m\u001b[39m seconds...\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     sleep(retry_wait_time)\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m (RateLimitError, Timeout) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    241\u001b[0m     time_left \u001b[39m=\u001b[39m max_retry_period \u001b[39m-\u001b[39m (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m+\u001b[39m retry_wait_time)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by the admin.\",\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message='''Engineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intented to be executed by the executor. Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor. If the result indicates that there is an error, fix the error and output the code again. Suggest the fill code instead of partial code or code changes. If the erroe can't be fixed or the task is not solved even after the code executed successfully, analyze the problem, revisit your assumptions, collect additional info that you need, and think of a different approach to try.\n",
    "    '''\n",
    ")\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Scientist. You follow an approved plan, You are able to categorize papers after seeing their abstracts printed. You don't write any code.\"\n",
    ")\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    llm_config=llm_config,\n",
    "    system_message='''Planner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval. The plan man involve an engineer who can write code and a scientist who doesn't write code. Explain the plan first. Be clear which step is to be performed by an engineer, and which step is to be performed by a scientist.'''\n",
    ")\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"paper\"}\n",
    ")\n",
    "critic = autogen.AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Critic. Double check the plan, claims, code from other agents and provide feedback. Check whether the plan includes adding verifiable info such as source url.\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor, critic],\n",
    "    messages=[],\n",
    "    max_round=50\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape arXiv for papers on LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Review the scraped data to identify the different domains in which LLM is applied. \n",
      "\n",
      "3. Engineer: Modify the script to categorize the papers based on the domains identified by the scientist.\n",
      "\n",
      "4. Engineer: Write a script to convert the categorized data into a markdown table.\n",
      "\n",
      "5. Scientist: Review the markdown table for accuracy and completeness.\n",
      "\n",
      "6. Engineer: Make any necessary revisions to the markdown table based on the scientist's feedback.\n",
      "\n",
      "7. Scientist: Give final approval of the markdown table.\n",
      "\n",
      "8. Engineer: Submit the final markdown table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to chat_manager):\n",
      "\n",
      "The plan seems comprehensive and well-structured. However, it lacks the inclusion of verifiable information such as source URLs. The Engineer should ensure that the script extracts the URL of each paper from arXiv. This will allow anyone reviewing the markdown table to verify the information and read the full papers if they wish. \n",
      "\n",
      "Also, the Scientist's role in identifying the different domains in which LLM is applied (Step 2) might be challenging without a clear definition or criteria for what constitutes a \"domain\". It would be helpful to clarify this in the plan. \n",
      "\n",
      "Lastly, the plan could be improved by including a step for the Scientist to review and approve the script's categorization method (Step 3). This would ensure that the categorization is scientifically sound and accurate.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Revised Plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape arXiv for papers on LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Define clear criteria for what constitutes a \"domain\" in the context of LLM applications.\n",
      "\n",
      "3. Scientist: Review the scraped data to identify the different domains in which LLM is applied, based on the criteria defined in step 2.\n",
      "\n",
      "4. Engineer: Modify the script to categorize the papers based on the domains identified by the scientist.\n",
      "\n",
      "5. Scientist: Review and approve the script's categorization method to ensure it is scientifically sound and accurate.\n",
      "\n",
      "6. Engineer: Write a script to convert the categorized data, including the source URLs, into a markdown table.\n",
      "\n",
      "7. Scientist: Review the markdown table for accuracy and completeness.\n",
      "\n",
      "8. Engineer: Make any necessary revisions to the markdown table based on the scientist's feedback.\n",
      "\n",
      "9. Scientist: Give final approval of the markdown table.\n",
      "\n",
      "10. Engineer: Submit the final markdown table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "To accomplish this task, we will use the arXiv API to fetch the papers. The API allows us to filter papers based on the time of submission and the subject area. We will filter for papers submitted in the last week and in the field of Computer Science (cs). We will then search the titles and abstracts of these papers for the term \"LLM\" or \"Language Model\". \n",
      "\n",
      "The arXiv API returns data in the Atom format, which is a type of XML. We will use the `feedparser` library to parse this data. We will then extract the relevant information (title, authors, abstract, link) for each paper and store it in a list of dictionaries.\n",
      "\n",
      "Finally, we will use the `tabulate` library to convert this list of dictionaries into a markdown table.\n",
      "\n",
      "Here is the Python code to accomplish this:\n",
      "\n",
      "```python\n",
      "import feedparser\n",
      "from datetime import datetime, timedelta\n",
      "from tabulate import tabulate\n",
      "\n",
      "# Define the date a week ago\n",
      "week_ago = (datetime.now() - timedelta(weeks=1)).strftime('%Y%m%d%H%M%S')\n",
      "\n",
      "# Define the URL for the arXiv API\n",
      "url = f\"http://export.arxiv.org/api/query?search_query=submittedDate:[{week_ago}+TO+*]+AND+cat:cs&sortBy=submittedDate&sortOrder=descending\"\n",
      "\n",
      "# Fetch and parse the data\n",
      "data = feedparser.parse(url)\n",
      "\n",
      "# Extract the relevant information for each paper\n",
      "papers = []\n",
      "for entry in data.entries:\n",
      "    if 'LLM' in entry.title or 'Language Model' in entry.title or 'LLM' in entry.summary or 'Language Model' in entry.summary:\n",
      "        papers.append({\n",
      "            'Title': entry.title,\n",
      "            'Authors': ', '.join(author.name for author in entry.authors),\n",
      "            'Abstract': entry.summary,\n",
      "            'Link': entry.link\n",
      "        })\n",
      "\n",
      "# Convert the list of dictionaries into a markdown table\n",
      "table = tabulate(papers, headers=\"keys\", tablefmt=\"pipe\")\n",
      "\n",
      "print(table)\n",
      "```\n",
      "\n",
      "This code will print a markdown table of papers on LLM applications from arXiv submitted in the last week. The table includes the title, authors, abstract, and link for each paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 3, in <module>\n",
      "    from tabulate import tabulate\n",
      "ModuleNotFoundError: No module named 'tabulate'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems like the Python environment doesn't have the `tabulate` module installed, which is used to create the markdown table. \n",
      "\n",
      "Let's modify the code to create the markdown table manually without using the `tabulate` module. Here is the revised Python code:\n",
      "\n",
      "```python\n",
      "import feedparser\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Define the date a week ago\n",
      "week_ago = (datetime.now() - timedelta(weeks=1)).strftime('%Y%m%d%H%M%S')\n",
      "\n",
      "# Define the URL for the arXiv API\n",
      "url = f\"http://export.arxiv.org/api/query?search_query=submittedDate:[{week_ago}+TO+*]+AND+cat:cs&sortBy=submittedDate&sortOrder=descending\"\n",
      "\n",
      "# Fetch and parse the data\n",
      "data = feedparser.parse(url)\n",
      "\n",
      "# Extract the relevant information for each paper\n",
      "papers = []\n",
      "for entry in data.entries:\n",
      "    if 'LLM' in entry.title or 'Language Model' in entry.title or 'LLM' in entry.summary or 'Language Model' in entry.summary:\n",
      "        papers.append({\n",
      "            'Title': entry.title,\n",
      "            'Authors': ', '.join(author.name for author in entry.authors),\n",
      "            'Abstract': entry.summary,\n",
      "            'Link': entry.link\n",
      "        })\n",
      "\n",
      "# Create the markdown table manually\n",
      "table = '| Title | Authors | Abstract | Link |\\n| --- | --- | --- | --- |\\n'\n",
      "for paper in papers:\n",
      "    table += f\"| {paper['Title']} | {paper['Authors']} | {paper['Abstract']} | {paper['Link']} |\\n\"\n",
      "\n",
      "print(table)\n",
      "```\n",
      "\n",
      "This code will print a markdown table of papers on LLM applications from arXiv submitted in the last week. The table includes the title, authors, abstract, and link for each paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "| Title | Authors | Abstract | Link |\n",
      "| --- | --- | --- | --- |\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mScientist\u001b[0m (to chat_manager):\n",
      "\n",
      "It appears that there were no papers on LLM applications submitted to arXiv in the field of Computer Science in the last week. The table is currently empty. \n",
      "\n",
      "Please note that the results may vary depending on the time of execution as new papers are constantly being submitted to arXiv. \n",
      "\n",
      "If you want to search for papers over a longer time period or in a different field, please let me know and I can adjust the parameters accordingly.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-21 19:04:52] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b551de0a1a6cbff5c18e3108a1e53f9e in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b551de0a1a6cbff5c18e3108a1e53f9e in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b551de0a1a6cbff5c18e3108a1e53f9e in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 22 Nov 2023 00:04:51 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '273', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '147700', 'x-ratelimit-remaining-tokens_usage_based': '147700', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '920ms', 'x-ratelimit-reset-tokens_usage_based': '920ms', 'x-request-id': 'b551de0a1a6cbff5c18e3108a1e53f9e', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829ceade3e7854b5-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Revised Plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape arXiv for papers on LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Define clear criteria for what constitutes a \"domain\" in the context of LLM applications.\n",
      "\n",
      "3. Scientist: Review the scraped data to identify the different domains in which LLM is applied, based on the criteria defined in step 2.\n",
      "\n",
      "4. Engineer: Modify the script to categorize the papers based on the domains identified by the scientist.\n",
      "\n",
      "5. Scientist: Review and approve the script's categorization method to ensure it is scientifically sound and accurate.\n",
      "\n",
      "6. Engineer: Write a script to convert the categorized data, including the source URLs, into a markdown table.\n",
      "\n",
      "7. Scientist: Review the markdown table for accuracy and completeness.\n",
      "\n",
      "8. Engineer: Make any necessary revisions to the markdown table based on the scientist's feedback.\n",
      "\n",
      "9. Scientist: Give final approval of the markdown table.\n",
      "\n",
      "10. Engineer: Submit the final markdown table.\n",
      "\n",
      "Please note that the results may vary depending on the time of execution as new papers are constantly being submitted to arXiv. If you want to search for papers over a longer time period or in a different field, please let me know and I can adjust the parameters accordingly.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "okay why dont you try for the past month\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure, we can modify the script to search for papers submitted in the last month instead of the last week. Here is the revised Python code:\n",
      "\n",
      "```python\n",
      "import feedparser\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Define the date a month ago\n",
      "month_ago = (datetime.now() - timedelta(weeks=4)).strftime('%Y%m%d%H%M%S')\n",
      "\n",
      "# Define the URL for the arXiv API\n",
      "url = f\"http://export.arxiv.org/api/query?search_query=submittedDate:[{month_ago}+TO+*]+AND+cat:cs&sortBy=submittedDate&sortOrder=descending\"\n",
      "\n",
      "# Fetch and parse the data\n",
      "data = feedparser.parse(url)\n",
      "\n",
      "# Extract the relevant information for each paper\n",
      "papers = []\n",
      "for entry in data.entries:\n",
      "    if 'LLM' in entry.title or 'Language Model' in entry.title or 'LLM' in entry.summary or 'Language Model' in entry.summary:\n",
      "        papers.append({\n",
      "            'Title': entry.title,\n",
      "            'Authors': ', '.join(author.name for author in entry.authors),\n",
      "            'Abstract': entry.summary,\n",
      "            'Link': entry.link\n",
      "        })\n",
      "\n",
      "# Create the markdown table manually\n",
      "table = '| Title | Authors | Abstract | Link |\\n| --- | --- | --- | --- |\\n'\n",
      "for paper in papers:\n",
      "    table += f\"| {paper['Title']} | {paper['Authors']} | {paper['Abstract']} | {paper['Link']} |\\n\"\n",
      "\n",
      "print(table)\n",
      "```\n",
      "\n",
      "This code will print a markdown table of papers on LLM applications from arXiv submitted in the last month. The table includes the title, authors, abstract, and link for each paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "| Title | Authors | Abstract | Link |\n",
      "| --- | --- | --- | --- |\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-21 19:06:00] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 17d0452cc7feed22e90bd183d96843ab in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 17d0452cc7feed22e90bd183d96843ab in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 17d0452cc7feed22e90bd183d96843ab in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 22 Nov 2023 00:06:00 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '137', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '146965', 'x-ratelimit-remaining-tokens_usage_based': '146965', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '1.214s', 'x-ratelimit-reset-tokens_usage_based': '1.214s', 'x-request-id': '17d0452cc7feed22e90bd183d96843ab', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829cec8baf4b54b5-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-21 19:06:12] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b90decbe96530e499187286ea78d8871 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b90decbe96530e499187286ea78d8871 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b90decbe96530e499187286ea78d8871 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 22 Nov 2023 00:06:12 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '1300', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '146965', 'x-ratelimit-remaining-tokens_usage_based': '146965', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '1.214s', 'x-ratelimit-reset-tokens_usage_based': '1.214s', 'x-request-id': 'b90decbe96530e499187286ea78d8871', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829ceccc9baa54b5-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-21 19:06:22] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 396f513ff98980b750b14bf9c1395cab in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 396f513ff98980b750b14bf9c1395cab in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 396f513ff98980b750b14bf9c1395cab in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 22 Nov 2023 00:06:22 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '125', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '146965', 'x-ratelimit-remaining-tokens_usage_based': '146965', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '1.214s', 'x-ratelimit-reset-tokens_usage_based': '1.214s', 'x-request-id': '396f513ff98980b750b14bf9c1395cab', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829ced159a1154b5-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-21 19:06:33] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID a9140be19cb234dd1288d2313b970cf0 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID a9140be19cb234dd1288d2313b970cf0 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID a9140be19cb234dd1288d2313b970cf0 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 22 Nov 2023 00:06:33 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '292', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '146965', 'x-ratelimit-remaining-tokens_usage_based': '146965', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '1.214s', 'x-ratelimit-reset-tokens_usage_based': '1.214s', 'x-request-id': 'a9140be19cb234dd1288d2313b970cf0', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829ced563d9054b5-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-21 19:06:59] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/autogen/oai/completion.py\", line 224, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/srivatsa/my_jupyter_noteboks/autogen/pyautogen/lib/python3.9/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 7ca3bc7ae7de06951f59fa1473523d60 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 7ca3bc7ae7de06951f59fa1473523d60 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 7ca3bc7ae7de06951f59fa1473523d60 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 22 Nov 2023 00:06:58 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '15512', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '146965', 'x-ratelimit-remaining-tokens_usage_based': '146965', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '1.214s', 'x-ratelimit-reset-tokens_usage_based': '1.214s', 'x-request-id': '7ca3bc7ae7de06951f59fa1473523d60', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829ced982f2c54b5-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "end discussion\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to chat_manager):\n",
      "\n",
      "The plan and code provided by the Engineer and Scientist are sound. The code correctly fetches papers from arXiv based on the specified criteria and generates a markdown table. However, it seems there were no papers on LLM applications submitted in the last week or month in the field of Computer Science. This could be due to the specific time frame or the specific field chosen. The results may vary if the time frame is extended or if other fields are included in the search. \n",
      "\n",
      "The plan could be improved by including a step to verify the results and adjust the search parameters if necessary. For example, if no papers are found, the script could automatically extend the search to a longer time frame or include more fields. \n",
      "\n",
      "Overall, the plan and code are well-structured and should work correctly given the right conditions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"\n",
    "find papers on LLM applications from arxiv in the last month, create a markdown table of different domains.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID f876ca1a8d04c1a1d90be7faa88f7d34 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 22 Nov 2023 00:02:25 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-f2f9oe2pksgfimcmikqvwbod', 'openai-processing-ms': '2168', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '150000', 'x-ratelimit-limit-tokens_usage_based': '150000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '148002', 'x-ratelimit-remaining-tokens_usage_based': '148002', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '798ms', 'x-ratelimit-reset-tokens_usage_based': '798ms', 'x-request-id': 'f876ca1a8d04c1a1d90be7faa88f7d34', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '829ce7411a9d36d8-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyautogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
